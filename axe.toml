# =============================================================================
# AXE Configuration File
#   Header for each setting section
#   APP
#   LOGGER - output setting
#   LSM - Log structured merge tree assumptions and settings
#   JOB - all job specific settings
#   LCM - Learned cost model specifics
#   LTune - Learned tuner specifics
#   LRSCHEDULERS - ML learning rate schduler kwargs
#   OPTIMIZERS - ML optimizer kwargs
#   LOSS - ML Loss function kwargs
# =============================================================================

# =============================================================================
# HEADER APP
#   List of jobs we want to run in axe/jobs folder
# =============================================================================
[app]
name = "AXE"
run = [
    # "create_lcm_data",
    # "train_lcm",
    # "create_ltuner_data",
    "train_ltuner"
]
random_seed = 2169
disable_tqdm = false

# =============================================================================
# HEADER LOGGER
#   Generic IO settings for experiments, saving data, etc
# =============================================================================
[log]
format = "[%(levelname)s][%(asctime)-15s][%(filename)s] %(message)s"
datefmt = '%d-%m-%y:%H:%M:%S'
level = "INFO"

# =============================================================================
# HEADER LSM
#   Generic LSM settings
# =============================================================================
[lsm]
# Policy will effect everything else down stream (e.g. choice of neural network
# architecture for learned cost model)
#   Tiering
#   Leveling
#   Classic - Considers both leveing and tiering
#   QHybrid - Levels 1 -> L = Q
#   Fluid - Levels 1 -> (L-1) = Q, Level L = Z
#   Kapacity - Each level has own K_i decision
policy = 'Kapacity'

[lsm.bounds]
max_considered_levels = 20                  # Max number of levels to consider
size_ratio_range = [2, 22]                  # low, high of size ratios to consider
page_sizes = [4, 8, 16]                     # KB pages
entry_sizes = [1024, 2048, 4096, 8192]      # bits
memory_budget_range = [5, 20]               # low, high, bits per element
selectivity_range = [1e-7, 1e-9]            # low, high
elements_range = [100000000, 1000000000]    # element range

# =============================================================================
# HEADER JOB
#   Settings for each individual job
# =============================================================================
# --------------------------------------
[job] # Common settings across all jobs
# --------------------------------------
use_gpu_if_avail = true

# --------------------------------------
[job.create_lcm_data]
# --------------------------------------
output_dir = "data/lcm/train_data/kaplsm"
num_samples = 1024
num_files = 4
num_workers = 0
overwrite_if_exists = false

# --------------------------------------
[job.train_lcm]
# --------------------------------------
max_epochs = 5
save_dir = "data/lcm/models/kaplsm_model"
no_checkpoint = true

data_split = 0.9
data_dir = "data/lcm/train_data/kaplsm"
batch_size = 8
shuffle = true
num_workers = 0

# Different loss functions to train via
#   MSE - Mean squared error
#   NMSE - Normalized mean squared error
#   MSLE - Mean squared log error
#   RMSE - Root mean square error
#   RMSLE - Root mean squared log error
#   Huber - Huber loss
loss_fn = "MSE"

# Supported optimizers
#   [SGD, Adam, Adagrad]
optimizer = "Adam"

# Learning rate schedulers
#   [CosineAnnealing, Exponential, Constant, None]
lr_scheduler = "Constant"

# --------------------------------------
[job.create_ltuner_data]
# --------------------------------------
output_dir = "data/ltuner/train_data/std"
num_samples = 1024
num_files = 4
num_workers = 0
overwrite_if_exists = false

# --------------------------------------
[job.train_ltuner]
# --------------------------------------
max_epochs = 5
save_dir = "data/ltuner/models/kaplsm"
# Learned cost model is our loss, input full path to checkpoint or model file
loss_fn_path = "data/lcm/models/kaplsm_model"
optimizer = "Adam"
lr_scheduler = "Constant"
no_checkpoint = true

data_split = 0.9
data_dir = "data/ltuner/train_data/std"
batch_size = 1024
shuffle = true
num_workers = 0

# =============================================================================
# HEADER LCM
#   Add configurations related to learned cost models
# =============================================================================
[lcm.model]
embedding_size = 8
hidden_length = 3
hidden_width = 32
decision_dim = 64
dropout = 0.0           # dropout percentage
norm_layer = "Batch"    # "Batch" or "Layer" norm

# Used only for classic models, generally smaller than embedding size
policy_embedding_size = 4

# =============================================================================
# HEADER LTUNER
#   Learned tuner module
# =============================================================================
[ltuner]
penalty_factor = 10

# kwargs specific to LTune models during forward pass
[ltuner.train_kwargs]
temp = 1
hard = false

[ltuner.test_kwargs]
temp = 0.01
hard = true

# -----------------------------------------------------------------------------
# HEADER LTUNER.MODEL
#   Model configurations
# -----------------------------------------------------------------------------
[ltuner.model]
hidden_length = 1
hidden_width = 64
dropout = 0                     # dropout percentage
norm_layer = "Batch"            # batch or layer norm
categorical_mode = "reinmax"    # reinmax or gumbel

# =============================================================================
# HEADER LRSCHEDULERS
#   Specific settings for any learning rate schedulers
# =============================================================================
[scheduler.CosineAnnealingLR]
T_max = 10
eta_min = 0.0001  # minimum learning rate

[scheduler.Exponential]
gamma = 0.9

# =============================================================================
# HEADER OPTIMIZERS
#   Settings for any optimizers
# =============================================================================
[optimizer.Adam]
lr = 0.001

[optimizer.SGD]
lr = 0.001

[optimizer.Adagrad]
lr = 0.001

# =============================================================================
# HEADER LOSS
#   Settings for individual loss functions
# =============================================================================
[loss.Huber]
reduction = 'sum'
delta = 10

[loss.MSE]
reduction = 'mean'

