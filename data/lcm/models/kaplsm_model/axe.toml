[app]
name = "AXE"
run = [ "create_lcm_data", "train_lcm",]
random_seed = 2169
disable_tqdm = false

[log]
format = "[%(levelname)s][%(asctime)-15s][%(filename)s] %(message)s"
datefmt = "%d-%m-%y:%H:%M:%S"
level = "INFO"

[lsm]
policy = "Kapacity"

[job]
use_gpu_if_avail = true

[lsm.bounds]
max_considered_levels = 20
size_ratio_range = [ 2, 22,]
page_sizes = [ 4, 8, 16,]
entry_sizes = [ 1024, 2048, 4096, 8192,]
memory_budget_range = [ 5, 20,]
selectivity_range = [ 1e-7, 1e-9,]
elements_range = [ 100000000, 1000000000,]

[job.create_lcm_data]
output_dir = "data/lcm/train_data/kaplsm"
num_samples = 1024
num_files = 4
file_prefix = "data"
num_workers = 2
overwrite_if_exists = true

[job.train_lcm]
max_epochs = 5
save_dir = "data/lcm/models/kaplsm_model"
no_checkpoint = true
data_split = 0.9
data_dir = "data/lcm/train_data/kaplsm"
batch_size = 8
shuffle = true
num_workers = 4
loss_fn = "MSE"
optimizer = "Adam"
lr_scheduler = "Constant"

[lcm.model]
embedding_size = 8
hidden_length = 3
hidden_width = 32
decision_dim = 64
dropout = 0.0
norm_layer = "Batch"
policy_embedding_size = 4

[scheduler.CosineAnnealingLR]
T_max = 10
eta_min = 0.0001

[scheduler.Exponential]
gamma = 0.9

[optimizer.Adam]
lr = 0.001

[optimizer.SGD]
lr = 0.001

[optimizer.Adagrad]
lr = 0.001

[loss.Huber]
reduction = "sum"
delta = 10

[loss.MSE]
reduction = "mean"
